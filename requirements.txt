# TODO When updating flash-attn or torch in the future, make sure to update the version in the Dockerfile 
torch<=2.2.1
scipy
packaging
sentencepiece
datasets<=2.17.0  # 2.18.0 throws many warnings and caching seems not working in distributed setup
deepspeed>=0.10.0
accelerate>=0.21.0,<0.23.0  # 0.23.0 will cause an incorrect learning rate schedule when using deepspeed, which is likely caused by https://github.com/huggingface/accelerate/commit/727d624322c67db66a43c559d8c86414d5ffb537
peft>=0.4.0
bitsandbytes>=0.41.1
evaluate>=0.4.0
tokenizers>=0.13.3
protobuf
transformers==4.40
openai>=1.0.0
tiktoken
rouge_score
tensorboard
wandb
gradio==3.50.2
termcolor
jsonlines
unidic-lite
einops
flash-attn==2.5.2 # should really only be in dockerfile. Local env often doesn't have GPUs
auto-gptq
fire
alpaca-eval==0.6
# for human eval web app
flask
vllm @ git+https://github.com/vllm-project/vllm@fbf152d976e655f8734561452a1036e63081ccfd # todo bump to next release for easier install
openpyxl
# for ifeval
nltk
langdetect
immutabledict
ai2-olmo>=0.3.0 # for compatibility with OLMo models in transition to Transformers integration