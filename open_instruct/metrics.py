from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import PreTrainedModel
import torch
from typing import List, Dict, Tuple, Any
from abc import abstractmethod
import numpy as np
# from datasets import load_metric
import evaluate

from gem_metrics.msttr import MSTTR
from gem_metrics.ngrams import NGramStats
from gem_metrics.texts import Predictions

from tqdm import tqdm
import copy
import rouge
import pdb





class BaseMetric:
    @abstractmethod
    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[List[str]],
        meta_infos: List[Dict[str, Any]] = None,
        model: PreTrainedModel = None,
        split_name: str = None,
    ):
        """
        Returns a dict where key is the metric name and value is again a dict consisting of tuple of individual scores (if any) and corpus level score

        eg. {
            metric_name: (individual_scores, corpus_level_score)
            "metric_1": ([0.5, 0.5, 0.8], 0.1)
        }

        """
        raise NotImplementedError


class CRLHFEvaluationMetric:
    def __init__(self) -> None:
        # Initialize the required metrics
        self._bleu_metric = evaluate.load("bleu")
        self._sacrebleu_metric = evaluate.load("sacrebleu")
        self._rouge_metric = evaluate.load("rouge")
        self._diversity_metrics = DiversityMetrics()

        # Min-Max normalization ranges for scaling scores
        self._ranges = {
            "bleu": (0.0, 0.004676984628247904),
            "sacrebleu": (5.330883519077344e-06, 0.11063353821330388),
            # "sacrebleu": (0, 100),
            "rouge": (0.0, 0.014568765083436451),
            "max_pred_length": (9, 183),
            "vocab_size": (41, 32227),
            "unique": (314, 36880),
        }

    def _normalize(self, score: float, metric: str) -> float:
        """Normalize a score using the predefined min-max range."""
        _min, _max = self._ranges[metric]
        return (score - _min) / (_max - _min)

    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[str],
        meta_infos: List[Dict[str, Any]] = None,
    ) -> torch.Tensor:
        """
        Compute CRLHFEval scores for a batch of generated texts and return them as a PyTorch tensor.

        Args:
            prompt_texts (List[str]): List of prompt texts (not directly used but kept for API consistency).
            generated_texts (List[str]): List of texts generated by the model.
            reference_texts (List[str]): List of single reference texts for each generated text.
            meta_infos (List[Dict[str, Any]], optional): Additional metadata (not used here).

        Returns:
            torch.Tensor: A tensor of size (batch_size,) containing the CRLHFEval scores for each generated text.
        """
        # Convert references into the required format (list of lists)
        reference_texts = [[ref] for ref in reference_texts]
        # print(f'generated_texts: {[generated_texts]}')

        # Compute lexical metrics
        bleu_scores = []
        sacrebleu_scores = []
        rouge_scores = []
        for prediction, reference in zip(generated_texts, reference_texts):
            # BLEU

            # print(f'prediction, reference: {prediction}, {reference}')
            bleu_result = self._bleu_metric.compute(predictions=[prediction], references=reference)["bleu"]
            bleu_scores.append(self._normalize(bleu_result, "bleu"))

            # SacreBLEU
            sacrebleu_result = self._sacrebleu_metric.compute(predictions=[prediction], references=reference)["score"]
            sacrebleu_scores.append(self._normalize(sacrebleu_result, "sacrebleu"))

            # ROUGE (ROUGE-2)
            rouge_result = self._rouge_metric.compute(predictions=[prediction], references=reference)["rouge2"]
            rouge_scores.append(self._normalize(rouge_result, "rouge"))

        # Average lexical scores
        lexical_scores = [(b + s + r) / 3 for b, s, r in zip(bleu_scores, sacrebleu_scores, rouge_scores)]
        # print(f'bleu_score: {bleu_scores}, sacrebleu_score: {sacrebleu_scores}, rouge_score: {rouge_scores}, lexical_score: {lexical_scores}')
        # Compute diversity metrics
        diversity_metrics = self._diversity_metrics.compute(
            prompt_texts=prompt_texts,
            generated_texts=generated_texts,
            reference_texts=reference_texts,
        )
        max_pred_length_scores = []
        vocab_size_scores = []
        unique_scores = []
        for prediction in [generated_texts]:
            max_pred_length = diversity_metrics["diversity_metrics/max_pred_length-nopunct"]
            vocab_size = diversity_metrics["diversity_metrics/vocab_size-3-nopunct"]
            unique = diversity_metrics["diversity_metrics/unique-3"]
            

            max_pred_length_scores.append(self._normalize(max_pred_length[1], "max_pred_length"))
            vocab_size_scores.append(self._normalize(vocab_size[1], "vocab_size"))
            unique_scores.append(self._normalize(unique[1], "unique"))

        # Average diversity scores
        diversity_scores = [(m + v + u) / 3 for m, v, u in zip(max_pred_length_scores, vocab_size_scores, unique_scores)]
        # Combine lexical and diversity scores
        crlhf_scores = [(lex + div) / 2 for lex, div in zip(lexical_scores, diversity_scores)]
        # print(f'bleu_score: {bleu_scores}, sacrebleu_score: {sacrebleu_scores}, rouge_score: {rouge_scores}')
        # print(f'maxpredlength_score: {max_pred_length_scores}, vocabsize_score: {vocab_size_scores}, unique_score: {unique_scores}')
        # print(f'crlhfeval_score: {crlhf_scores}')
        return torch.tensor(crlhf_scores, dtype=torch.float32)


class SacreBLEUMetric(BaseMetric):
    def __init__(self, **args) -> None:
        super().__init__()
        self._args = args
        self._metric = evaluate.load("sacrebleu")

    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[List[str]],
        meta_infos: List[Dict[str, Any]] = None,
        model: PreTrainedModel = None,
        split_name: str = None,
    ) -> Tuple[List[float], float]:

        metric_results = self._metric.compute(
            predictions=generated_texts, references=reference_texts, **self._args
        )
        bleu_score = metric_results["score"] / 100
        metric_dict = {"lexical/sacrebleu": (None, bleu_score)}
        return metric_dict



class RougeMetric(BaseMetric):
    def __init__(self, use_single_ref: bool = True) -> None:
        super().__init__()
        self._metric = evaluate.load("rouge")
        self._use_single_ref = use_single_ref

    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[List[str]],
        meta_infos: List[Dict[str, Any]] = None,
        model: PreTrainedModel = None,
        split_name: str = None,
    ):
        if self._use_single_ref:
            # TBD: this is required for CNN/DM dataset, without this we get low scores
            # TBD: needs investigation
            ref_texts = [ref[0] for ref in reference_texts]
        else:
            ref_texts = reference_texts

        metric_results = self._metric.compute(
            predictions=generated_texts, references=ref_texts, use_stemmer=True
        )
        # print('we are in rougemetric, ', metric_results)
        # print(generated_texts, ref_texts)
        score_keys = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
        metric_dict = {}
        for rouge_type in score_keys:
            rouge_score = metric_results[rouge_type]
            metric_dict[f"lexical/rouge_{rouge_type}"] = (None, rouge_score)
        return metric_dict


# class BERTScoreMetric(BaseMetric):
#     def __init__(self, language: str) -> None:
#         super().__init__()
#         self._metric = evaluate.load("bertscore")
#         self._language = language
#         # since models are loaded heavily on cuda:0, use the last one to avoid memory
#         self._last_gpu = f"cuda:{torch.cuda.device_count() - 1}"

#     def compute(
#         self,
#         prompt_texts: List[str],
#         generated_texts: List[str],
#         reference_texts: List[List[str]],
#         meta_infos: List[Dict[str, Any]] = None,
#         model: PreTrainedModel = None,
#         split_name: str = None,
#     ) -> Tuple[List[float], float]:
#         with torch.no_grad():
#             metric_results = self._metric.compute(
#                 predictions=generated_texts,
#                 references=reference_texts,
#                 lang=self._language,
#                 device=self._last_gpu,
#             )
#             bert_scores = metric_results["f1"]
#             corpus_level_score = np.mean(bert_scores)
#             metric_dict = {"semantic/bert_score": (bert_scores, corpus_level_score)}
#             return metric_dict


class BLEUMetric(BaseMetric):
    def __init__(self) -> None:
        super().__init__()
        self._metric = evaluate.load("bleu")

    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[List[str]],
        meta_infos: List[Dict[str, Any]] = None,
        model: PreTrainedModel = None,
        split_name: str = None,
    ) -> Tuple[List[float], float]:

        tokenized_predictions = []
        tokenized_reference_texts = []
        for prediction, refs in zip(generated_texts, reference_texts):
            tokenized_prediction = prediction.split()
            tokenized_refs = [ref.split() for ref in refs]
            tokenized_predictions.append(tokenized_prediction)
            tokenized_reference_texts.append(tokenized_refs)

        try:
            # print(generated_texts, reference_texts)
            metric_results = self._metric.compute(
                predictions=generated_texts, references=reference_texts
            )
            # print(metric_results, 'metric results')
            bleu_score = metric_results["bleu"]
            metric_dict = {"lexical/bleu": (None, bleu_score)}
            return metric_dict
        except Exception as e:
            return {"lexical/bleu": (None, "n/a")}


# class BLEURTMetric(BaseMetric):
#     def __init__(self, config_name: str = None) -> None:
#         super().__init__()
#         self._metric = evaluate.load("bleurt", config_name=config_name)

#     def compute(
#         self,
#         prompt_texts: List[str],
#         generated_texts: List[str],
#         reference_texts: List[List[str]],
#         meta_infos: List[Dict[str, Any]] = None,
#         model: PreTrainedModel = None,
#         split_name: str = None,
#     ) -> Tuple[List[float], float]:
#         metric_results = self._metric.compute(
#             predictions=generated_texts, references=reference_texts
#         )
#         corpus_score = np.mean(metric_results["scores"])
#         metric_dict = {"semantic/bleurt": (metric_results["scores"], corpus_score)}
#         return metric_dict


def get_generated_and_predictions(
    prompt_texts: List[str],
    generated_texts: List[str],
    reference_texts: List[List[str]],
    split_name: str,
):
    split_name = "" if split_name is None else split_name
    preds = {}
    refs = {}
    for ix, (prompt_text, gen_text, ref_text) in enumerate(
        zip(prompt_texts, generated_texts, reference_texts)
    ):
        preds[split_name + prompt_text] = [gen_text]
        refs[split_name + prompt_text] = ref_text
    return preds, refs


def get_individual_scores(
    prompt_texts: List[str], split_name: str, scores_dict: Dict[str, float]
):
    split_name = "" if split_name is None else split_name
    scores = []
    for prompt_text in prompt_texts:
        scores.append(scores_dict.get(split_name + prompt_text, "n/a"))
    return scores


class DiversityMetrics(BaseMetric):
    def __init__(self, window_size: int = 100) -> None:
        self._msttr_metric = MSTTR(window_size=window_size)
        self._n_gram_metric = NGramStats()

    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[List[str]],
        meta_infos: List[Dict[str, Any]] = None,
        model: PreTrainedModel = None,
        split_name: str = None,
    ) -> Tuple[List[float], float]:

        predictions = Predictions(data={"filename": "", "values": generated_texts})
        diversity_metrics = {}
        msttr_metrics = self._msttr_metric.compute(None, predictions)
        n_gram_metrics = self._n_gram_metric.compute(None, predictions)
        # print('in DiversityMetrics class')
        # print(f'type of generated_texts: {type(generated_texts)}, {generated_texts}')

        for key, value in msttr_metrics.items():
            diversity_metrics[f"diversity_metrics/{key}"] = (None, value)
        for key, value in n_gram_metrics.items():
            diversity_metrics[f"diversity_metrics/{key}"] = (None, value)

        return diversity_metrics


class IntentAccuracy:
    def __init__(self) -> None:
        # Initialize the tokenizer and model
        self._tokenizer = AutoTokenizer.from_pretrained(
            "rajkumarrrk/roberta-daily-dialog-intent-classifier"
        )
        self._model = AutoModelForSequenceClassification.from_pretrained(
            "rajkumarrrk/roberta-daily-dialog-intent-classifier"
        )
        self._tokenizer.model_max_length = 2048  # Or another appropriate value


    
    def get_model(self):
        return self._model
        
    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[str] = None,  # Not required for intent classification
        meta_infos: List[Dict[str, Any]] = None,
        model=None,
        split_name: str = None,
    ) -> torch.Tensor:
        """
        Compute intent probabilities for a batch of generated texts and return them as a tensor.

        Args:
            prompt_texts (List[str]): List of prompts (dialog histories).
            generated_texts (List[str]): List of generated texts (responses).
            reference_texts (List[List[str]], optional): Not used here.
            meta_infos (List[Dict[str, Any]]): List of metadata dictionaries, each containing the target intent.

        Returns:
            torch.Tensor: A tensor of size (batch_size,) containing probabilities for the target intent class.
        """
        def get_input_for_classifier(prompt, generated_text):
            # Combine the last utterance of the prompt with the generated response
            history = prompt.split("[EOU]")
            history = [utt.strip() for utt in history if utt.strip()]
            last_utterance = history[-1]
            input_text = last_utterance + " " + generated_text
            return input_text

        # Create input texts for the classifier
        input_texts = [
            get_input_for_classifier(prompt, gen)
            for prompt, gen in zip(reference_texts, generated_texts)
        ]

        # Tokenize input texts
        encoded = self._tokenizer(
            input_texts,
            return_tensors="pt",
            truncation=True,             # Truncate inputs to max_length
            padding=True,                # Pad inputs to the same length
            max_length=self._tokenizer.model_max_length  # Use model's max length
        )
        device = next(self._model.parameters()).device  # Get the device of the model

        # Perform inference with the intent classifier
        with torch.no_grad():
            outputs = self._model(
                input_ids=encoded.input_ids.to(device),
                attention_mask=encoded.attention_mask.to(device),
            )
            # Extract logits and apply softmax to get probabilities
            probabilities = torch.softmax(outputs.logits, dim=1)

        # Get the probability for the target class
        target_probabilities = probabilities[:, 0]

        # Return the probabilities as a tensor of shape (batch_size,)
        return target_probabilities

class MeteorMetric:
    def __init__(self) -> None:
        # Initialize the METEOR metric from the evaluate library
        self._metric = evaluate.load("meteor")

    def compute(
        self,
        prompt_texts: List[str],
        generated_texts: List[str],
        reference_texts: List[str],
        meta_infos: List[Dict[str, Any]] = None,
    ) -> torch.Tensor:
        """
        Compute METEOR scores for a batch of generated texts and return them as a PyTorch tensor.

        Args:
            prompt_texts (List[str]): List of prompt texts (not used for METEOR but included for API consistency).
            generated_texts (List[str]): List of texts generated by the model.
            reference_texts (List[List[str]]): List of reference texts for comparison, where each generated text has its own list of references.
            meta_infos (List[Dict[str, Any]], optional): Additional metadata (not used here).

        Returns:
            torch.Tensor: A tensor of size (batch_size,) containing the METEOR scores for each generated text.
        """
        # Compute individual METEOR scores for each generated text and its corresponding reference(s)
        batch_scores = []
        for prediction, reference in zip(generated_texts, reference_texts):
            # Compute METEOR score for a single prediction-reference pair
            score = self._metric.compute(predictions=[prediction], references=[reference])["meteor"]
            batch_scores.append(score)

        # Convert the list of scores into a PyTorch tensor
        return torch.tensor(batch_scores, dtype=torch.float32)
    

if __name__ == "__main__":
    # # Initialize the CRLHFEvaluationMetric
    # crlhf_metric = CRLHFEvaluationMetric()

    # # Example prompts, generated texts, and references
    # prompt_texts = [
    #     "The cat was looking for a place to rest.",
    #     "The dog was searching for its bone.",
    # ]
    # generated_texts = [
    #     "The cat is sitting on the mat.",
    #     "The dog is lying under the table.",
    # ]
    prompt_texts = [""]
    gen_texts = ["Hello there general kenobi", "foo bar foobar"]
    reference_texts = ["Hello there general kenobi", "foo bar foobar"]
    # generated_texts = 'The Australian Business and Trade Commission, representing large Australian businesses, has opposed the plan, saying that the system would cost $750 per person, with the majority of the cost likely being borne by large companies.<|end_of_text|>!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!'
    # reference_texts = [
    #     "The cat sat on the mat.",
    #     "The dog rested under the table.",
    # ]

    metric = CRLHFEvaluationMetric()
    value = metric.compute(generated_texts=gen_texts, reference_texts=reference_texts, prompt_texts=None)
    print(f'value: {value}')
    # # Compute the CRLHF metric
    # crlhf_scores = crlhf_metric.compute(
    #     prompt_texts=prompt_texts,
    #     generated_texts=generated_texts,
    #     reference_texts=reference_texts,
    # )

    # # Print the results
    # print("CRLHFEval Metric Scores (Tensor):", crlhf_scores)
    # print("Tensor Shape:", crlhf_scores.shape)

    # diversity = DiversityMetrics()

    # values = diversity.compute(prompt_texts= None, reference_texts= None, generated_texts=generated_texts)

    # # for key in values:
    # #     print(f'key: {key}, value: {values[key]}')


    # max_pred_length = values["diversity_metrics/max_pred_length-nopunct"]
    # vocab_size = values["diversity_metrics/vocab_size-3-nopunct"]
    # unique = values["diversity_metrics/unique-3"]

    # print(max_pred_length)
    # print(vocab_size)
    # print(unique)